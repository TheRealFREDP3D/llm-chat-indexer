# LLM Provider API Keys - Add your API keys for the LLM service you want to use
GOOGLE_API_KEY=your_google_api_key_here        # Required for Gemini models
# OPENAI_API_KEY=your_openai_key_here          # Required for OpenAI models
# ANTHROPIC_API_KEY=your_anthropic_key_here    # Required for Anthropic models
# GROQ_API_KEY=your_groq_jey_here              # Required for Groq models

# Add any supported by LiteLLM or that are OpenAI compatible


# Configuration
BASE_DIR=.                                      # Root directory for the application
OUTPUT_DIR=./output                             # Directory where all output files will be stored
SUMMARY_FILENAME=chat_summaries.md              # Filename for the generated markdown summaries
INDEX_FILENAME=chat_index.json                  # Filename for the JSON index of all processed chats
LLM_PROVIDER=gemini/gemini-2.0-flash            # LLM provider and model to use for summarization
LLM_API_KEY=
SUPPORTED_FILE_EXTENSIONS=.txt,.md,.json,.html,.csv  # File types that can be processed by the indexer
MAX_TOPIC_KEYWORDS=5                            # Maximum number of keywords to extract per chat (higher values provide more detailed but potentially noisier topics)
LOG_LEVEL=INFO                                  # Logging verbosity (DEBUG, INFO, WARNING, ERROR, CRITICAL)
LOG_FILE=chat_indexer.log                       # File where logs will be written
